

















import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

get_ipython().run_line_magic("matplotlib", " inline")

# Зафиксируем случайность, чтобы каждый раз получалось одно и тоже
np.random.seed(seed=42)


from sklearn import datasets

# загружаем датасет с цифрами
X, y = datasets.load_digits(return_X_y=True)

print("Экземпляров: {}\nРазмер изображения: {}x{}".format(X.shape[0], np.sqrt(X.shape[1]), np.sqrt(X.shape[1])))


plt.figure(figsize=(16, 6))
width = int(np.sqrt(X.shape[1]))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(X[i,:].reshape([width,width]), cmap='gray')





sns.countplot(y)





def euclidian(x):  # норма в Евклидовом пространстве
    return np.sqrt(np.sum(x**2, axis=1))

def euclidian_metric(a, b):  # реализуем Евклидову метрику через норму
    return euclidian(a - b)





def _find_neighbours(k, y, distances):
    #############
    #  ВАШ КОД  #
    #############
    sorted_indices = np.argsort(distances)
    neighbours = y[sorted_indices][:k]
    neighbours_distances = distances[sorted_indices][:k]

    return neighbours, neighbours_distances


# sanity check
Y = np.arange(10)
dist = np.linspace(1, 10, 10)
_find_neighbours(3, Y, dist)  # должен вернуться кортеж из массивов [0, 1, 2] и [1, 2, 3], если вернулось что-то другое, то что-то не так реализовано





def _get_closest_classes(neighbours):
    #############
    #  ВАШ КОД  #
    #############
    classes, counts = np.unique(neighbours, return_counts=True)
    max_count = np.max(counts)
    best_classes = classes[counts == max_count]
    return best_classes


# sanity check
print(_get_closest_classes(np.asarray([1,2,3,2,2])))  # должна вернуться только "2"
print(_get_closest_classes(np.asarray([1,2,3,2,3])))  # должны вернуться "2" и "3"





def _choose_best_class(best_classes, neighbours, neighbouring_distances):
    min_mean_dist = np.inf
    best_class = None

    #############
    #  ВАШ КОД  #
    #############

    for cls in best_classes:
        indices = neighbours == cls
        mean_dist = np.mean(neighbouring_distances[indices])
        if mean_dist < min_mean_dist:
            min_mean_dist = mean_dist
            best_class = cls

    return best_class


# sanity check
_choose_best_class([1,2], np.array([1, 2, 1, 3, 2]), np.asarray([0.5, 1, 1, 8, 0.6]))  # если всё правильно, best_class должен быть "1"





# эта функция будет считать расстояния от новой точки new_x до всех точек в исходном датасете X и на основе расстояний вычислять принадлежность к классу
def _nearest_neighbours_classify(x, y, k, new_x, metric):
    distances = metric(x, new_x)  # считаем расстояния до классов

    neighbours, neighbouring_distances = _find_neighbours(k, y, distances)  # находим ровно k соседей этой точки
        
    best_classes = _get_closest_classes(neighbours)  # обнаруживаем классы, которые имеются среди соседей

    res = _choose_best_class(best_classes, neighbours, neighbouring_distances)  # выбираем наиболее релевантный класс по среднему расстоянию до него среди соседей

    return res
        
# а в этой функции мы повторяем это всё для каждого элемента нашей выборки. Короче говоря, обрабатываем сразу батч (на самом деле не сразу, а по одной точке, медленно, но понятно).
def nearest_neighbours_classify(x, y, k, x_pred, metric=euclidian_metric):
    res = np.zeros(x_pred.shape[0], dtype=y.dtype)
    for i in range(x_pred.shape[0]):
        res[i] = _nearest_neighbours_classify(x, y, k, x_pred[i], metric)
    return res





from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)

y_pred = nearest_neighbours_classify(X_train, y_train, 5, X_test)

print(accuracy_score(y_test, y_pred))





from sklearn.neighbors import KNeighborsClassifier # класс для kNN классификатора

from sklearn.model_selection import cross_val_score # метод для кросс-валидации данных

from sklearn.model_selection import KFold # алгоритм разбиения выборки на группы(фолды)
from sklearn.model_selection import StratifiedKFold # алгоритм разбиения выборки на стратифицированные группы(фолды)





clf = KNeighborsClassifier()

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

get_ipython().run_line_magic("time", " scores = cross_val_score(clf, X, y, cv=cv)")

print("Accuracy: {}".format(scores.mean()))





from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split

# разобъем датасет на train и test в пропорции 60/40
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.4)
y_test = y_test.astype('int')

clf = KNeighborsClassifier()
clf.fit(X_train, y_train) # обучим модель
get_ipython().run_line_magic("time", " y_pred = clf.predict(X_test).astype('int') # предскажем тэги на тестовой подвыборке")


print(classification_report(y_test, y_pred)) # напечатаем отчет о классификации





plt.figure(figsize=(12,12)) 
_ = sns.heatmap(confusion_matrix(y_test, y_pred), cmap=plt.cm.Blues, square=True, annot=True, fmt='.4g')








from sklearn.model_selection import GridSearchCV


np.arange(1, 10)





params = {
    "n_neighbors": np.arange(2, 10), 
    "p": [2,4]
}

search = GridSearchCV(KNeighborsClassifier(), params, n_jobs=2, 
                      cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42), verbose=2)
get_ipython().run_line_magic("time", " search.fit(X, y)")


x_ticks = ["{:02d}_n_neighbors={}, p={}".format(i, p['n_neighbors'], p['p']) for i,p in enumerate(search.cv_results_['params'])]

plt.figure(figsize=(16,8))
plt.plot(x_ticks, search.cv_results_['mean_test_score'])
_ =plt.xticks(rotation=90)

print("BEST: score={}, params={}".format(search.best_score_, search.best_params_))





clf = KNeighborsClassifier(n_neighbors=3, p=2)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test).astype('int')

print(classification_report(y_test, y_pred))

plt.figure(figsize=(12,12))
_ = sns.heatmap(confusion_matrix(y_test, y_pred), cmap=plt.cm.Blues, square=True, annot=True, fmt='.3g')


clf = KNeighborsClassifier(n_neighbors=3, p=2)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
get_ipython().run_line_magic("time", " scores = cross_val_score(clf, X, y, cv=cv)")
print("Accuracy: {}".format(scores.mean()))
# с настройками по умолчанию было 0.9838396055603897





colab = False
if colab:
    from google.colab import drive
    drive.mount('/content/drive')


if colab:
    df = pd.read_csv('/content/drive/My Drive/Data/dataset_191_wine.csv')
else:
    df = pd.read_csv('../data/dataset_191_wine.csv')
df.head()


X = df.drop(['class'], axis=1)
y = df['class']





cv = KFold(n_splits=5, shuffle=False) # фиксируем разбиения! Выключаем перемешивание для повтора результатов

k_vals = np.arange(1, 100, 1)
quality_by_k = [
    cross_val_score(KNeighborsClassifier(n_neighbors=k), X, y, cv=cv).mean()
    for k in k_vals
]

print("Best K = {}".format(k_vals[np.argmax(quality_by_k)]))
plt.plot(k_vals, quality_by_k)





df.describe()


from sklearn.preprocessing import scale
X_scaled = scale(X) # включим масштабирование
cv = KFold(n_splits=5, shuffle=False)

k_vals = np.arange(1, 100, 1)
quality_by_k = [
    cross_val_score(KNeighborsClassifier(n_neighbors=k), X_scaled, y, cv=cv).mean()
    for k in k_vals
]

print("Best K = {}".format(k_vals[np.argmax(quality_by_k)]))
plt.plot(k_vals, quality_by_k)





X_scaled = scale(X)
cv = KFold(n_splits=5, shuffle=False)

p_vals = np.linspace(1, 10, 100)
quality_by_p = [
    cross_val_score(KNeighborsClassifier(n_neighbors=16, metric='minkowski', p=p), X_scaled, y, cv=cv).mean()
    for p in p_vals
]

print(p_vals[np.argmax(quality_by_p)])
plt.plot(p_vals, quality_by_p)



